#include <cublas_v2.h>
#include <cuda_runtime.h>
#include <vector>
#include <iostream>
#include <fstream>

#define BLOCK_SIZE 32
#define TOL 1e-5f

void checkCudaError(cudaError_t err, const char *msg) {
    if (err != cudaSuccess) {
        std::cerr << msg << " CUDA ERROR: " << cudaGetErrorString(err) << std::endl;
        exit(EXIT_FAILURE);
    }
}
void checkCublasError(cublasStatus_t status, const char *msg) {
    if (status != CUBLAS_STATUS_SUCCESS) {
        std::cerr << msg << " CUBLAS ERROR: " << status << std::endl;
        exit(EXIT_FAILURE);
    }
}
__global__ void mysgemm_v1(int M, int N, int K, float alpha, float *A, float *B,float beta, float* C) {
    int gx = blockIdx.x * blockDim.x + threadIdx.x;
    int gy = blockIdx.y* blockDim.y + threadIdx.y;
    //C[gy][gx]
    //å†™è¿™ä¸ªçŸ©é˜µç›¸ä¹˜ é¦–å…ˆéœ€è¦æ˜ç¡®çš„æ˜¯ å·²ç»æœ‰äº†gx gyäº†ï¼Œé‚£ä¹ˆè¯¥çº¿ç¨‹å°±è´Ÿè´£ Aä¸­ç¬¬ gyè¡Œï¼Œä¸Bä¸­ç¬¬gxåˆ—çš„


    if (gx >= N || gy >= M) return;
    float tmp = 0.0f;       //ç´¯åŠ å™¨ï¼Œä¿å­˜ A çš„ä¸€è¡Œ ä¸ B çš„ä¸€åˆ— ç‚¹ç§¯ç»“æœ
    for (int i = 0; i < K; i++) {
        tmp += A[gy*K+i]*B[gx+i*N];
    }
    C[gy*N+gx]= alpha*tmp + beta*C[gy*N+gx];

}
//__global__ void mysgemm_v2(int M, int N, int K, float alpha, float *A, float *B,float beta, float* C) {
//    //å†™è¿™ä¸ªçš„æ€»ä½“æ€è·¯ä¾¿æ˜¯ä¸å¦‚ç›´æ¥ç±»æ¯” ä¹‹å‰çº¿ç¨‹ (gx,gy)è´Ÿè´£ C[gy][gx]
//    //è¿™é‡ŒåŒæ ·ï¼Œçº¿ç¨‹å—ï¼ˆblockIdx.x, blockIdx.yï¼‰è´Ÿè´£ C[blockIdx.y, blockIdx.x] è¿™ä¸ª C åˆ†å—çš„å¤§å°å°±æ˜¯ [BM,BN]
//    //è¦ä½¿ç”¨å…±äº«æ˜¾å­˜
//    int bx = blockIdx.x;
//    int by = blockIdx.y;
//
//    const int BM = BLOCK_SIZE;
//    const int BN = BLOCK_SIZE;
//    const int BK = BLOCK_SIZE;
//
//    int tx = threadIdx.x % BN;
//    int ty = threadIdx.y / BN; //å› ä¸ºè¿™ä¸ªçº¿ç¨‹å—æ˜¯ä¸€ç»´çš„ï¼Œæ‰€ä»¥åªæœ‰threadIdx.xæ²¡æœ‰ yï¼Œä½†æ˜¯çŸ©é˜µå¿«è¿ç®—æ˜¯2ç»´çš„ï¼Œæ‰€ä»¥éœ€è¦æ˜ å°„
//
//    __shared__ float As[BM*BK];
//    __shared__ float Bs[BN*BK];//æ³¨æ„æ­¤å¤„ åŠ¨æ€åªèƒ½è®¾ç½®ä¸€ä¸ªï¼Œé™æ€å¯ä»¥æœ‰å¤šä¸ªï¼
//
//    //è¿™é‡Œéœ€è¦æ˜ç¡®çš„æ˜¯ï¼Œçº¿ç¨‹å—è´Ÿè´£çš„æ˜¯ C[by][bx] è¿™ä¸ªåˆ†å—ï¼Œæ¯ä¸ªåˆ†å—å¤§å°æ˜¯BM * BN
//
//    // ğŸ”¹ A çš„å½“å‰å­å—èµ·å§‹ä½ç½®ï¼š
//    A = &A[by*BM*K] ;//æ­¤å¤„çœ‹pptç”»çš„å›¾å§
//    B = &B[bx*BN];
//    C = &C[by*BM*N+bx*BN];
//
//    // â—æ¯ä¸ªçº¿ç¨‹è¦è®¡ç®— C å­å—ä¸­ä¸€ä¸ªå…·ä½“çš„å…ƒç´  Csub[ty, tx]
//    // å› æ­¤æ¯ä¸ªçº¿ç¨‹æœ€ç»ˆè¦ç´¯åŠ  BK æ¬¡ä¹˜æ³•ç»“æœã€‚
//    float tmp = 0.f;
//    //è¿™å•ä¸ªçº¿ç¨‹è¿˜è¦å¹²ä»€ä¹ˆå‘¢ï¼Ÿ ä¸€ä¸ªçº¿ç¨‹å—æ±‚çš„æ˜¯C[bx][by]  ï¼Œæ¯ä¸ªçº¿ç¨‹å—å¤§å°BM BN,  AçŸ©é˜µçš„å­å—ä¸ºBM*BK ç„¶è€Œå…¶æ•´è¡Œä¸ºBM*K
//    for (int k = 0; k < K; k += BK) {
//        //æ­¤å¤„çœ‹pptç»˜å›¾ä¸­ç»¿è‰²é‚£å—ï¼Œ è¿™é‡Œåº”è¯¥å­˜å‚¨çš„æ˜¯ç»¿è‰²çš„ä¸€éƒ¨åˆ†
//        //çœ‹ä»£ç ä¸æ˜¯ å­˜çš„æ˜¯BM*BK  BK*BNè¿™ä¹ˆå¤§çš„
//        //é‚£å°±è¦å®šä½ å­˜å“ªä¸ªå—ï¼Ÿ
//        //çœ‹pptå›¾ï¼Œçº¿ç¨‹å—ï¼ˆbx,byï¼‰è´Ÿè´£C[bx][by] ï¼Œ
//        //æ¯ä¸ªçº¿ç¨‹æ¬è¿ä¸€ä¸ªå…ƒç´ ï¼Œæ€»å…±æ¬è¿ä¸€å¥—BM*BK  BK*BNï¼Œ
//        //é¦–å…ˆç°åœ¨ å…¨å±€çš„A  B å·²ç»æŒ‡åˆ°æ­£ç¡®ä½ç½®äº†ï¼Œ  ç°åœ¨åªéœ€è¦ç®¡ty txè¡Œäº†ï¼Œ
//        //æ“¦äº† è¿™é‡ŒBM BNã€€BKä¸€æ ·å¤§ ï¼Œ
//        As[ty*BK+tx] = A[ty*K+tx];
//        Bs[ty*BN+tx] = B[ty*N+tx];
//        __syncthreads();//ç¡®ä¿ä¸€ä¸ªçº¿ç¨‹å—ä¸­çš„æ‰€æœ‰çº¿ç¨‹éƒ½æ¬å®Œäº†æ‰å¾€ä¸‹ä¹‹ä¹ æ€§
//        A += BK;
//        B += BK*N;//è¿™é‡Œå°±æ˜¯æŒ‡é’ˆæŒªåŠ¨å¤šå°‘ä¸ªå•ä½
//        for (int i = 0; i < BK; i++) {
//            tmp += As[ty*BK+i]*Bs[tx+i*BN];  //Aå­å—çš„tyè¿™è¡Œ  Bå­å—çš„txè¿™åˆ—å‚ä¸è®¡ç®—
//        }
//        __syncthreads(); //ç­‰æ‰€æœ‰çº¿ç¨‹éƒ½ç®—å®Œè¿™é‡Œ
//    }
//    C[ty*N+tx] = alpha*tmp + beta*C[ty*N+tx];
//}

__global__ void mysgemm_v2(int M, int N, int K, float alpha, float *A, float *B,float beta, float* C) {
    //é¦–å…ˆæ˜ç¡® bx byè´Ÿè´£è®¡ç®—C[by][bx],æ¯ä¸ªCåˆ†å—å¤§å°ä¸ºBM*BN
    int bx = blockIdx.x;
    int by = blockIdx.y;
    const int BM = BLOCK_SIZE;
    const int BN = BLOCK_SIZE;
    const int BK = BLOCK_SIZE;
    int tx = threadIdx.x % BN;
    int ty = threadIdx.y / BN;

    __shared__ float As[BM*BK];
    __shared__ float Bs[BN*BK];

    //è¦é¦–å…ˆå®šä½åˆ°A  B  Cçš„èµ·ç‚¹ï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿ
    //è¿™é‡Œè¦æŒ‡å‘å„è‡ªçš„èµ·ç‚¹
    A=&A[by*K*BM];
    B=&B[bx*BN];
    C=&C[by*N*BM+bx*BN];
    float tmp = 0.0f;
    for(int k=0;k<K;k+=BK){
        //å†™åˆ°è¿™è¦è€ƒè™‘ ä»€ä¹ˆå‘¢ï¼Ÿ è¦è€ƒè™‘æ•°æ®æ¬è¿äº†
        As[ty*BK+tx]  = A[ty*K+bx];
        Bs[ty*BN+tx]  = B[ty*N+tx];
        __syncthreads(); //ç­‰æ‰€æœ‰çº¿ç¨‹éƒ½æ¬è¿å®Œ
        A+=BK;
        B+=BK*N;
        for(int i=0;i<BK;i++){
            tmp += As[ty*BK+i]*Bs[tx+i*BN];
        }
    }
//    C[ty*N+tx] = alpha*tmp + beta*C[ty*N+tx];
//    æˆ‘æ„Ÿè§‰è¿™è¡Œåº”è¯¥æ˜¯
    C[ty*BN+tx] = alpha*tmp + beta*C[ty*BN+tx]; //å› ä¸ºå·²ç»æŒ‡å‘åˆ†å—èµ·å§‹åœ°å€äº†
}
template<const int BM,const int BN, const int BK, const int TM, const int TN>
__global__ void  mysgemm_v4(int M, int N, int K, float alpha, float *A, float *B,float beta, float* C) {
    //é¦–å…ˆæ˜ç¡® bx byè´Ÿè´£è®¡ç®—C[by][bx],æ¯ä¸ªCåˆ†å—å¤§å°ä¸ºBM*BN
    int bx = blockIdx.x;
    int by = blockIdx.y;

    int block_row_thread = BN/TN; //æ¨ªå‘æœ‰å¤šå°‘tile
    int block_col_thread = BM/TM; //æˆ‘å’‹æ„Ÿè§‰ è¿™å‘½åæœ‰ç‚¹åäº†å‘¢ï¼Ÿ
    int thread_num = block_row_thread*block_col_thread;

    int tx = (threadIdx.x % block_row_thread )*TN;
    int ty = (threadIdx.x % block_col_thread )*TM;

    __shared__ float As[BM*BK];
    __shared__ float Bs[BN*BK];  //ä¸Šè¾¹å€’æ˜¯å¹³å¸¸ å®¹æ˜“ç†è§£
    A = &A[by*BM*K];
    B = &B[bx*BN];
    C = &C[by*BM*N+bx*BN];   //è¿™é‡Œè¿˜æ˜¯åŒæ ·ç†è§£ï¼Œä¸€ä¸ªçº¿ç¨‹å—è´Ÿè´£æ¬è¿ä¸€ä¸ªC[by][bx],  è´Ÿè´£æ¬è¿Açš„ä¸€è¡Œ ï¼Œ Bçš„ä¸€åˆ—


    //ä¸‹è¾¹è¯¥å¦‚ä½•æ¬è¿å‘¢ï¼Ÿ æ¬è¿æ˜¯æ‰€æœ‰çº¿ç¨‹éƒ½å»æ¬ï¼Œç„¶åæ¯ä¸ªæ•°æ®æ¯æ¬¡å…ˆæ¬è¿ä¸€ä¸ªï¼Œ å¾ªç¯a_tile_strideæ¬¡æ•°  æ­¤å¤„è¦çœ‹ç¬¬3é¡µppt
    int a_tile_row  = threadIdx.x / BK; //ç›¸å½“äºæ¬è¿Açš„å­å—çš„ ç¬¬å‡ è¡Œ
    int a_tile_col =  threadIdx.x % BK; //ç›¸å½“äºæ¬è¿Açš„å­å—çš„ ç¬¬å‡ åˆ—
    int a_tile_stride = thread_num / BK;  //è¡¨ç¤ºå¾ªç¯å‡ æ¬¡

    int b_tile_row  = threadIdx.x / BN; //ç›¸å½“äºæ¬è¿Bçš„å­å—çš„ ç¬¬å‡ è¡Œ
    int b_tile_col =  threadIdx.x % BN; //ç›¸å½“äºæ¬è¿Bçš„å­å—çš„ ç¬¬å‡ åˆ—
    int b_tile_stride = thread_num / BN;  //è¡¨ç¤ºå¾ªç¯å‡ æ¬¡

    float tmp[TM][TN] = {0.0f}; //è¿™é‡Œè¿˜çœŸä¸æ˜ç™½äº† ï¼Œä¸ºä»€ä¹ˆè¿™é‡Œç´¢å¼•æ˜¯TM TNï¼Ÿéš¾é“è¿™é‡Œæ˜¯ä¸ªå£°æ˜äº†TM TNçš„äºŒç»´æ•°ç»„ï¼Ÿ ç¡®å®æ˜¯äºŒç»´æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åˆå§‹åŒ–ä¸º0.0f

    //è¿™é‡Œæ³¨æ„çœ‹å›¾2ï¼Œè¿˜æ˜¯ç®—ä¸€ä¸ªé»„è‰²å—ï¼Œæ‰€ä»¥è¦å¾ªç¯Kæ¬¡æ¬è¿è®¡ç®—
    for(int k=0;k<K;k+=BK){
        //å¼€å§‹æ¬è¿å­å—  å¦‚ä½•å®šä½å‘¢ï¼Ÿæˆ‘è§‰å¾—ç”¨ threadIdx.xå°±å¯ä»¥å•Šï¼Ÿ
//        for (int i =0; i<BM; i+=a_tile_stride){ //æ­¤æ—¶çš„iä¸ºç¬¬å‡ æ¬¡æ¬è¿
//            As[threadIdx.x + i * a_tile_stride*BK] = A[threadIdx.x + i* a_tile_stride *BK];
//        }
//        for (int i =0; i<BN; i+=b_tile_stride){ //æ­¤æ—¶çš„iä¸ºç¬¬å‡ æ¬¡æ¬è¿
//            Bs[threadIdx.x + i * b_tile_stride*BN] = B[threadIdx.x + i* b_tile_stride *BN];
//
//        }
    //æ—¢ç„¶chatgptè¯´ä¸Šè¿°ä¸è¡Œï¼Œ é‚£åªèƒ½ç”¨a_tile_row  a_tile_coläº†
        for(int i=0;i<BM;i+=a_tile_stride){
            //è¿™é‡Œè¿™ä¸ªå¾ªç¯ ç›¸å½“äºiå°±æ˜¯a_tile_strideäº†
            //å³è¾¹ä¸ºä»€ä¹ˆæ˜¯Kå‘¢ï¼Ÿå¯ä»¥çœ‹ç¬¬ä¸‰é¡µppt é‚£æ˜¯ä¸æ˜¯è¯´AçŸ©é˜µæŒ‡å‘ &A[by*BM*K]ä¸ºèµ·ç‚¹çš„åˆ†å—ï¼Œå°±æ˜¯ä¸ºäº†a_tile_colè¿™ä¸ªç´¢å¼•ã€‚å¯¹åº”è¯¥è¯´åŸºæœ¬æ­£ç¡®
            As[(a_tile_row + i) * BK + a_tile_col] = A[(a_tile_row+i)*K + a_tile_col];
        }
        for(int i=0;i<BN;i+=b_tile_stride){
            Bs[(b_tile_row+ i)*BN  + b_tile_col] =  B[(b_tile_row+i)*N + b_tile_col];
        }

        }
        __syncthreads();
        A+= BK;
        B+=BK*N; //è¿™ä¸ªæŒªçš„æ˜¯é»„è‰²é‚£ä¸ªçš„ä½ç½®

        //è¿™é‡Œå¼€å§‹è®¡ç®—  æ­¤æ—¶åˆå¾—çœ‹ç¬¬äºŒå¼ PPTäº†  ç›¯äº†ä¸€ä¼š é‚£çº¢è‰²çš„å››ä¸ªæ ¼å­çš„tmpæ˜¯ç´¯åŠ è€Œæ¥çš„
        //é‚£ä¹ˆæ”¹å¦‚ä½•å®šä½åˆ°As  Bsä¸­å‘¢ï¼Ÿ  çªç„¶æƒ³èµ·æ¥ çŸ¥é“ ty txäº† å³ çŸ¥é“åœ¨Cä¸­çš„èµ·å§‹åœ°å€äº†ï¼Œé‚£ä¹ˆå¯ä»¥æ ¹æ® TM  TNçš„ç´¢å¼•ç®—ä¸€ä¸‹å‘—
        //å¼€å§‹ç´¢å¼•æ˜¯ty tx
//        for(int j=0;j<TM;j++){
//            for(int i=0;i<TN;i++){
//                for(int k=0;k<BK;k++){
//                    tmp[j][i] +=  As[(ty+j)*BK+k]*Bs[tx+i + k*BN];
//                }
//            }
//        }
        for(int i=0;i<BK;i++){  //è¿™ä¸€å±‚æ˜¯å¹²ä»€ä¹ˆçš„æ¥ï¼Ÿ éå†BKçš„å®½åº¦  ä¹˜ç§¯å’Œå˜›
            for(int j=0;j<TM;j++){
                for(int l =0; l<TN ; l++){
                    tmp[j][l] += As[(ty+j)*BK+i]*Bs[tx+l+i*BN];
                }
            }
        }
        __syncthreads();
    //æŠŠtmpç»“æœ å†™å›å¯¹åº”çš„ä½ç½®
    for(int j=0;j<TM;j++){
        for(int l=0;l<TN;l++){
            C[(ty+j)*N+tx+l] = alpha*tmp[j][l] + beta*C[(ty+j)*N+tx+l];
        }
    }

    }
#define CEIL_DIV(M, N) ((M) + (N) - 1) / (N)
int main()
{
    std::vector<int> sizes = {128,256,512,1024,2048,4096,8192};
    std::ofstream csv_file("/cuda_code/tmp/sgemm_benchmark_v3.csv");

    for ( auto N :sizes)
    {
        std::cout << "Testing size: " << N << std::endl;
        size_t size = N*N*sizeof(float);
        float *A = (float *)malloc(size);
        float *B = (float *)malloc(size);
        float *C_cublas = (float *)malloc(size);
        float *C_v1 = (float *)malloc(size);
        float *d_A,*d_B,*d_C_v1;
        checkCudaError(cudaMalloc(&d_A,size),"cudaMalloc d_A failed");
        checkCudaError(cudaMalloc(&d_B,size),"cudaMalloc d_B failed");
        checkCudaError(cudaMalloc(&d_C_v1,size),"cudaMalloc d_C_v1 failed");

        bool out_of_memory = false;
        try{
            for (int i = 0; i < N*N; i++) {
                A[i] = 1.0f;
                B[i] = 2.0f;
            }
            checkCudaError(cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice),
                           "cudaMemcpy A to device failed");
            checkCudaError(cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice),
                           "cudaMemcpy B to device failed");
            cublasHandle_t handle;
            checkCublasError(cublasCreate(&handle), "cublasCreate failed");

            float alpha = 1.0f;
            float beta = 0.0f;

            cudaEvent_t start, stop;
            checkCudaError(cudaEventCreate(&start), "cudaEventCreate(start) failed");
            checkCudaError(cudaEventCreate(&stop), "cudaEventCreate(stop) failed");

            // warmup
            int warpup_time = 10;  // çƒ­èº«æ¬¡æ•°
            for (int i = 0; i < warpup_time; ++i) {
                checkCublasError(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, N, N, N,
                                             &alpha, d_B, N, d_A, N, &beta, d_C_v1, N),
                                 "cublasSgemm failed");
            }
            cudaDeviceSynchronize();
            // cuBLAS SGEMM
            int repeat_time = 5;
            checkCudaError(cudaEventRecord(start),
                           "cudaEventRecord(start cublas) failed");
            for (int i = 0; i < repeat_time; ++i) {
                checkCublasError(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, N, N, N,
                                             &alpha, d_B, N, d_A, N, &beta, d_C_v1, N),
                                 "cublasSgemm failed");
            }

            checkCudaError(cudaEventRecord(stop),
                           "cudaEventRecord(stop cublas) failed");
            checkCudaError(cudaEventSynchronize(stop),
                           "cudaEventSynchronize cublas failed");

            float cublas_time = 0;
            checkCudaError(cudaEventElapsedTime(&cublas_time, start, stop),
                           "cudaEventElapsedTime cublas failed");

            // æ‹·è´ cuBLAS ç»“æœ
            checkCudaError(cudaMemcpy(C_cublas, d_C_v1, size, cudaMemcpyDeviceToHost),
                           "cudaMemcpy C_cublas failed");

            // mysgemm_v1
            checkCudaError(cudaMemset(d_C_v1, 0, size), "cudaMemset d_C_v1 failed");
//            dim3 threads(BLOCK_SIZE, BLOCK_SIZE);//æ­¤å¤„BLOCK_SIZEä¸º32 cudaä¸­æ²¡æœ‰Dim2è¿™ä¸ªç±»å‹ ï¼Œå®é™…æ˜¯ï¼ˆBS,BS,1ï¼‰ ã€‚è¿™ä¸€è¡Œ åªæ˜¯å®šä¹‰äº† block çš„å†…éƒ¨çº¿ç¨‹å¸ƒå±€ï¼Œ
//            dim3 blocks((N + threads.x - 1) / threads.x,
//                        (N + threads.y - 1) / threads.y);
//            for (int i = 0; i < warpup_time; ++i) {
//                mysgemm_v1<<<blocks, threads>>>(N, N, N, alpha, d_A, d_B, beta, d_C_v1);
//            }
//            cudaDeviceSynchronize();
//            checkCudaError(cudaEventRecord(start),
//                           "cudaEventRecord(start v1) failed");
//            for (int i = 0; i < repeat_time; ++i) {
//                mysgemm_v1<<<blocks, threads>>>(N, N, N, alpha, d_A, d_B, beta, d_C_v1);
//            }


            //mysgemm_v2==========================
//            dim3 blockDim(1024);
//            dim3 gridDim(CEIL_DIV(N, 32), CEIL_DIV(N, 32));
//
//
//            for (int i = 0; i < warpup_time; ++i) {
//                mysgemm_v2
//                <<<gridDim, blockDim>>>(N, N, N, alpha, d_A, d_B, beta, d_C_v1);
//            }
//
//            cudaDeviceSynchronize();
//            checkCudaError(cudaMemset(d_C_v1, 0, size), "cudaMemset d_C_v1 failed");
//
//            checkCudaError(cudaEventRecord(start),
//                           "cudaEventRecord(start v1) failed");
//
//            for (int i = 0; i < repeat_time; ++i) {
//                mysgemm_v2
//                <<<gridDim, blockDim>>>(N, N, N, alpha, d_A, d_B, beta, d_C_v1);
//            }
//

            //mysgemm_v2===========================


            //mysgemm_v4===========================

            dim3 blockDim(256);
            dim3 gridDim(CEIL_DIV(N, 128), CEIL_DIV(N, 128));

            for (int i = 0; i < warpup_time; ++i) {
                mysgemm_v4<128, 128, 8, 8, 8>
                <<<gridDim, blockDim>>>(N, N, N, alpha, d_A, d_B, beta, d_C_v1);
            }


            cudaDeviceSynchronize();
            checkCudaError(cudaMemset(d_C_v1, 0, size), "cudaMemset d_C_v1 failed");

            checkCudaError(cudaEventRecord(start),
                           "cudaEventRecord(start v1) failed");

            for (int i = 0; i < repeat_time; ++i) {
                mysgemm_v4<128, 128, 8, 8, 8>
                <<<gridDim, blockDim>>>(N, N, N, alpha, d_A, d_B, beta, d_C_v1);
            }


            //mysgemm_v4===========================





            checkCudaError(cudaEventRecord(stop), "cudaEventRecord(stop v1) failed");
            checkCudaError(cudaEventSynchronize(stop),
                           "cudaEventSynchronize v1 failed");

            float v1_time = 0;
            checkCudaError(cudaEventElapsedTime(&v1_time, start, stop),
                           "cudaEventElapsedTime v1 failed");

            // æ‹·è´æ‰‹å†™ kernel ç»“æœ
            checkCudaError(cudaMemcpy(C_v1, d_C_v1, size, cudaMemcpyDeviceToHost),
                           "cudaMemcpy C_v1 failed");
            // ç»“æœæ¯”è¾ƒ
            int error_count = 0;
            for (int i = 0; i < N * N && error_count < 10; ++i) {
                if (fabsf(C_cublas[i] - C_v1[i]) > TOL) {
                    error_count++;
                }
            }
            float cublas_gflops =repeat_time * 2.0f * N * N * N / (cublas_time * 1e6f);  // GFlops
            float v1_gflops =repeat_time * 2.0f * N * N * N / (v1_time * 1e6f);  // GFlops
            csv_file << N << "," << cublas_gflops << "," << v1_gflops << ","
                     << (error_count == 0 ? "1" : "0") << std::endl;

            // é‡Šæ”¾èµ„æº
            cublasDestroy(handle);
            cudaEventDestroy(start);
            cudaEventDestroy(stop);
            cudaFree(d_A);
            cudaFree(d_B);
            cudaFree(d_C_v1);

            free(A);
            free(B);
            free(C_cublas);
            free(C_v1);
        }
        catch (...){
            std::cerr << "Out of memory or error during testing size: " << N
                      << std::endl;
            out_of_memory = true;
        }
        if (!out_of_memory) {
            std::cout << "Finished size: " << N << std::endl;
        } else {
            csv_file << N << ",OOM,OOM,0" << std::endl;
        }
    }
    csv_file.close();

//    std::cout << "Benchmark completed. Results saved to 'sgemm_benchmark.csv'"
//              << std::endl;
    return 0;
}